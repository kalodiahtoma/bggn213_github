---
title: "class 09"
author: "Kalodiah Toma (A07606689)"
format: pdf
---

```{r}
read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv")
wisc.df<-read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

Q1. How many observations are in this dataset?
569
```{r}
dim(wisc.df)
```
Q2. How many of the observations have a malignant diagnosis?
```{r}
diagnosis <- as.factor(wisc.df[,1])
```



```{r}
colnames(wisc.df)
```
Q3. How many variables/features in the data are suffixed with _mean?
10

```{r}
grep("mean", colnames(wisc.df))
```

```{r}
grep("mean", colnames(wisc.df), value =T)
colnames(wisc.df)
```
```{r}
wisc.data <- wisc.df[,-1]
colMeans(wisc.data)

apply(wisc.data,2,sd)
```
```{r}
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr)

#wisc.pr$x
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44%
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
3
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
7
```{r}
biplot(wisc.pr)
```
```{r}
plot(wisc.pr$x, col = diagnosis,
     xlab = "PC1", ylab = "PC2")
```
```{r}
plot(wisc.pr$x[,c(1,3)], col = diagnosis,  
     xlab = "PC1", ylab = "PC3")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
PC1 and PC3 clusters are closer togehter than PC1 and PC2 clusters. 
```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
library(ggplot2)
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
total_var <- sum(pr.var)
pve <- pr.var / total_var
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```


```{r}
table(diagnosis)
```
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.
-0.26085376

```{r}
wisc.pr$rotation[,1]
```

```{r}
d.pc<-dist(wisc.pr$x[,1:3])
wisc.pr.hc<-hclust(d.pc, method="ward.D2")
plot(wisc.pr.hc)
grps<-cutree(wisc.pr.hc, k=2)
table(grps)
```
Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
19

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
```{r}
table(diagnosis)
table(diagnosis, grps)
grps
```
Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
Average, cluster branches are longer
```{r}
data.scaled <- (wisc.data)
data.dist <- (data.scaled)
wisc.hclust <- (data.dist)
wisc.pr.hc<-hclust(d.pc, method="ward.D2")
plot(wisc.pr.hc)
wisc.pr.hc2<-hclust(d.pc, method="single")
plot(wisc.pr.hc2)
wisc.pr.hc3<-hclust(d.pc, method="complete")
plot(wisc.pr.hc3)
wisc.pr.hc4<-hclust(d.pc, method="average")
plot(wisc.pr.hc4)
```
```{r}
grps <- cutree(wisc.pr.hc, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
dplot<-read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv")
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
plot(wisc.pr.hclust)

```
```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
plot(wisc.pr.hclust.clusters)
```
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```
Q13. How well does the newly created model with four clusters separate out the two diagnoses?
It does a good job a clustering since each diagnosis has two larger clusters. It is easier to cluster with two fours.

Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.
The false negative in the hcluster condition are 24 whereas the false negative in the in the kmeans is 37. Having lower false negatives, during cancer diagnosis could be life ending.  
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
wisc.km.hclust <- kmeans(wisc.pr$x[,1:7], 2)
table(wisc.km.hclust$cluster, diagnosis)
```
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```


Q16. Which of these new patients should we prioritize for follow up based on your results?
Group 2 are the M and need more follow up, but group 1 is B. 




